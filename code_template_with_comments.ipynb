{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Multiclass Classification using Keras and TensorFlow on Food-101 Dataset**\n![alt text](https://www.vision.ee.ethz.ch/datasets_extra/food-101/static/img/food-101.jpg)","metadata":{"_uuid":"41a6777d5e67dc652f57ce9681b4c44dc44152be","id":"uNaVQGQ9tQRr"}},{"cell_type":"markdown","source":"### **Installing Prerequisites**","metadata":{"_uuid":"201bfa1a371439fbb44da6ef4e8d232bcca0b465","id":"Xa07tVPbP7Cu"}},{"cell_type":"code","source":"!pip install torch-lr-finder","metadata":{"execution":{"iopub.status.busy":"2021-05-21T10:04:08.521525Z","iopub.execute_input":"2021-05-21T10:04:08.521733Z","iopub.status.idle":"2021-05-21T10:04:14.673725Z","shell.execute_reply.started":"2021-05-21T10:04:08.521691Z","shell.execute_reply":"2021-05-21T10:04:14.672855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install keras-adabound","metadata":{"execution":{"iopub.status.busy":"2021-05-21T10:04:14.6763Z","iopub.execute_input":"2021-05-21T10:04:14.676581Z","iopub.status.idle":"2021-05-21T10:04:23.278173Z","shell.execute_reply.started":"2021-05-21T10:04:14.676534Z","shell.execute_reply":"2021-05-21T10:04:23.277235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install tensorboard","metadata":{"execution":{"iopub.status.busy":"2021-05-21T04:52:36.811838Z","iopub.execute_input":"2021-05-21T04:52:36.812071Z","iopub.status.idle":"2021-05-21T04:52:36.817692Z","shell.execute_reply.started":"2021-05-21T04:52:36.812026Z","shell.execute_reply":"2021-05-21T04:52:36.81704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch > 1.4","metadata":{"execution":{"iopub.status.busy":"2021-05-21T10:04:28.590745Z","iopub.execute_input":"2021-05-21T10:04:28.591024Z","iopub.status.idle":"2021-05-21T10:04:33.178043Z","shell.execute_reply.started":"2021-05-21T10:04:28.590978Z","shell.execute_reply":"2021-05-21T10:04:33.177265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2021-05-21T10:04:23.279566Z","iopub.execute_input":"2021-05-21T10:04:23.279838Z","iopub.status.idle":"2021-05-21T10:04:28.589439Z","shell.execute_reply.started":"2021-05-21T10:04:23.279791Z","shell.execute_reply":"2021-05-21T10:04:28.588496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Importing Packages**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom fastai.vision import *\nfrom fastai.metrics import error_rate\nimport matplotlib.image as img\nfrom PIL import Image\n%matplotlib inline\nimport numpy as np\nfrom collections import defaultdict\nimport collections\nfrom shutil import copy\nfrom shutil import copytree, rmtree\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing import image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport os\nimport random\nimport time\nimport datetime\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\n# from tensorflow.keras.applications.nasnet import NasNetLarge\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow import keras\nfrom tensorflow.keras import models\nimport cv2\nfrom keras_adabound import AdaBound\nimport tensorboard as tb\nfrom torchvision.models.resnet import resnet50\nimport torchvision\nimport torch\nimport torchvision.models as modelst\n#from torch.utils.tensorboard import SummaryWriter\n# import torchvision.models as models\n#from tensorboardX import SummaryWriter\nfrom torchsummary import summary","metadata":{"_uuid":"dd82702380162e9587a0eae2f644dae2764f93c8","execution":{"iopub.status.busy":"2021-05-21T10:04:33.179503Z","iopub.execute_input":"2021-05-21T10:04:33.179766Z","iopub.status.idle":"2021-05-21T10:04:37.739492Z","shell.execute_reply.started":"2021-05-21T10:04:33.179721Z","shell.execute_reply":"2021-05-21T10:04:37.738808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip uninstall tensorboard --yes","metadata":{"execution":{"iopub.status.busy":"2021-05-21T04:52:46.190417Z","iopub.execute_input":"2021-05-21T04:52:46.190684Z","iopub.status.idle":"2021-05-21T04:52:46.199973Z","shell.execute_reply.started":"2021-05-21T04:52:46.190638Z","shell.execute_reply":"2021-05-21T04:52:46.199244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if GPU is enabled\nprint(tf.__version__)\nprint(tf.test.gpu_device_name())","metadata":{"_uuid":"70f06e9a535b5f32ad9d927fc00e767dd72f17dd","id":"JOZZbCDoP-Hy","outputId":"99f6277e-0b8e-4541-a9b4-cce1a98f5b57","execution":{"iopub.status.busy":"2021-05-21T04:52:46.202622Z","iopub.execute_input":"2021-05-21T04:52:46.202876Z","iopub.status.idle":"2021-05-21T04:52:46.214833Z","shell.execute_reply.started":"2021-05-21T04:52:46.202834Z","shell.execute_reply":"2021-05-21T04:52:46.21399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"c134497b2239d3fa5f377fb5aebf1887f627233f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/input/food-101/","metadata":{"execution":{"iopub.status.busy":"2021-05-21T08:37:24.521001Z","iopub.execute_input":"2021-05-21T08:37:24.521308Z","iopub.status.idle":"2021-05-21T08:37:24.540507Z","shell.execute_reply.started":"2021-05-21T08:37:24.52125Z","shell.execute_reply":"2021-05-21T08:37:24.539676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Use the below two blocks of code if running this file on Google Colab","metadata":{}},{"cell_type":"code","source":"# Should be used while working on Colab\n\n# # Helper function to download data and extract\n# def get_data_extract():\n#   if \"food-101\" in os.listdir():\n#     print(\"Dataset already exists\")\n#   else:\n#     print(\"Downloading the data...\")\n#     !wget http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n#     print(\"Dataset downloaded!\")\n#     print(\"Extracting data..\")\n#     !tar xzvf food-101.tar.gz\n#     print(\"Extraction done!\")","metadata":{"_uuid":"339ff95b36c15cfb16a9316d2a19cc89aaaf7160","id":"f88XvEBTQBS9","execution":{"iopub.status.busy":"2021-05-21T04:52:46.231833Z","iopub.execute_input":"2021-05-21T04:52:46.232319Z","iopub.status.idle":"2021-05-21T04:52:46.238893Z","shell.execute_reply.started":"2021-05-21T04:52:46.232138Z","shell.execute_reply":"2021-05-21T04:52:46.238098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Download data and extract it to folder\n# Uncomment this below line if you are on Colab\n\n#get_data_extract()","metadata":{"_uuid":"05fd6dae2dde446b3bd4a17f81a202391f9328d3","id":"O7kY0v23QJGO","outputId":"edc14855-bf55-4938-a875-af7d24729ea6","execution":{"iopub.status.busy":"2021-05-21T04:52:46.240207Z","iopub.execute_input":"2021-05-21T04:52:46.240801Z","iopub.status.idle":"2021-05-21T04:52:46.250871Z","shell.execute_reply.started":"2021-05-21T04:52:46.240491Z","shell.execute_reply":"2021-05-21T04:52:46.250211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the extracted dataset folder\n!ls food-101/","metadata":{"_uuid":"7ceb7f04d55f3d1510a03868713e773e7df17046","id":"7wJ_OH1DQyrd","outputId":"bda2768c-24b9-4962-b7eb-fb3b07d6fad4","execution":{"iopub.status.busy":"2021-05-21T04:52:46.252324Z","iopub.execute_input":"2021-05-21T04:52:46.252874Z","iopub.status.idle":"2021-05-21T04:52:46.891852Z","shell.execute_reply.started":"2021-05-21T04:52:46.252538Z","shell.execute_reply":"2021-05-21T04:52:46.890927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**meta** folder contains the text files - train.txt and test.txt  \n**train.txt** contains the list of images that belong to training set  \n**test.txt** contains the list of images that belong to test set  \n**classes.txt** contains the list of all classes of food","metadata":{"_uuid":"2decce84b3d9ffa491d19ded5357805061604749","id":"Ld4DOKjzSdns"}},{"cell_type":"markdown","source":"### **Visualize random image from each of the 101 classes**","metadata":{"_uuid":"4af37d46992f3b2aa7b74efcb5102751b167914e","id":"motIEZu_TVih"}},{"cell_type":"code","source":"# # Visualize the data, showing one image per class from 101 classes\n# rows = 17\n# cols = 6\n# fig, ax = plt.subplots(rows, cols, figsize=(25,25))\n# fig.suptitle(\"Showing one random image from each class\", y=1.05, fontsize=24) # Adding  y=1.05, fontsize=24 helped me fix the suptitle overlapping with axes issue\n# data_dir = \"food-101/images/\"\n# foods_sorted = sorted(os.listdir(data_dir))\n# food_id = 0\n# for i in range(rows):\n#   for j in range(cols):\n#     try:\n#       food_selected = foods_sorted[food_id] \n#       food_id += 1\n#     except:\n#       break\n#     if food_selected == '.DS_Store':\n#         continue\n#     food_selected_images = os.listdir(os.path.join(data_dir,food_selected)) # returns the list of all files present in each food category\n#     food_selected_random = np.random.choice(food_selected_images) # picks one food item from the list as choice, takes a list and returns one random item\n#     img = plt.imread(os.path.join(data_dir,food_selected, food_selected_random))\n#     ax[i][j].imshow(img)\n#     ax[i][j].set_title(food_selected, pad = 10)\n    \n# plt.setp(ax, xticks=[],yticks=[])\n# plt.tight_layout()\n# # https://matplotlib.org/users/tight_layout_guide.html\n","metadata":{"_uuid":"da6910e8bb064b76c17e07f0a2e0e23ebdefbbfa","id":"Jfif27Pr5KEn","outputId":"a451ddd4-2beb-4d36-eb00-000ed2f23286","execution":{"iopub.status.busy":"2021-05-21T04:52:46.905598Z","iopub.execute_input":"2021-05-21T04:52:46.905828Z","iopub.status.idle":"2021-05-21T04:52:46.913036Z","shell.execute_reply.started":"2021-05-21T04:52:46.905785Z","shell.execute_reply":"2021-05-21T04:52:46.912194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Split the image data into train and test using train.txt and test.txt**\n* We first read the textfile with the list of images that belong to either train or test\n* We then create a train or test directory\n* Finally, we move the images from the ***Images*** folder to the ***train/test*** folder","metadata":{"_uuid":"7a8300cde1146c50e672079b44346e723d813702","id":"KIgareCETmct"}},{"cell_type":"code","source":"# Helper method to split dataset into train and test folders\ndef prepare_data(filepath, src,dest):\n  classes_images = defaultdict(list)\n  with open(filepath, 'r') as txt:\n      paths = [read.strip() for read in txt.readlines()]\n      for p in paths:\n        food = p.split('/')\n        classes_images[food[0]].append(food[1] + '.jpg')\n\n  for food in classes_images.keys():\n    print(\"\\nCopying images into \",food)\n    if not os.path.exists(os.path.join(dest,food)):\n      os.makedirs(os.path.join(dest,food))\n    for i in classes_images[food]:\n      copy(os.path.join(src,food,i), os.path.join(dest,food,i))\n  print(\"Copying Done!\")","metadata":{"_uuid":"d6c87e52c1af18e3243f1cb72b5834941828b286","id":"xB0XMUX_5KMQ","execution":{"iopub.status.busy":"2021-05-21T08:37:34.20292Z","iopub.execute_input":"2021-05-21T08:37:34.203184Z","iopub.status.idle":"2021-05-21T08:37:34.20971Z","shell.execute_reply.started":"2021-05-21T08:37:34.203138Z","shell.execute_reply":"2021-05-21T08:37:34.208678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare train dataset by copying images from food-101/images to food-101/train using the file train.txt\n%cd /\nprint(\"Creating train data...\")\nprepare_data('/kaggle/input/food-101/food-101/meta/train.txt', '/kaggle/input/food-101/food-101/images', 'train')","metadata":{"_uuid":"786cdb3d60ebcdddb4c2875f9e0b4508c5210fc1","id":"LSgcYcqy5KUd","outputId":"7e65498b-bcd8-4209-87e8-bdc1a8c37b4e","execution":{"iopub.status.busy":"2021-05-21T08:37:42.214674Z","iopub.execute_input":"2021-05-21T08:37:42.214943Z","iopub.status.idle":"2021-05-21T08:53:17.45522Z","shell.execute_reply.started":"2021-05-21T08:37:42.214897Z","shell.execute_reply":"2021-05-21T08:53:17.454489Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare test data by copying images from food-101/images to food-101/test using the file test.txt\nprint(\"Creating test data...\")\nprepare_data('/kaggle/input/food-101/food-101/meta/test.txt', '/kaggle/input/food-101/food-101/images', 'test')","metadata":{"_uuid":"ba85577e0ef10c768e000ecf32dee36566d52599","id":"JI65wZgT5Kb-","outputId":"2e71e6bc-43de-4ea3-f5d6-a660c4a3dc42","execution":{"iopub.status.busy":"2021-05-21T08:53:17.456452Z","iopub.execute_input":"2021-05-21T08:53:17.456706Z","iopub.status.idle":"2021-05-21T08:56:06.35046Z","shell.execute_reply.started":"2021-05-21T08:53:17.456663Z","shell.execute_reply":"2021-05-21T08:56:06.349529Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Used a simple lambda function to attach the list of images with a .txt extension and then use it to count the number of matching files in the ***images*** folder","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/food-101/food-101/meta/train.txt', header=None).apply(lambda x : x + '.jpg')\nprint(\"Total number of samples in train folder: \", len(train_df))\ntrain_image_list = ImageItemList.from_df(train_df, '/kaggle/input/food-101/food-101/images')","metadata":{"execution":{"iopub.status.busy":"2021-05-21T08:56:06.351641Z","iopub.execute_input":"2021-05-21T08:56:06.35192Z","iopub.status.idle":"2021-05-21T08:56:06.71901Z","shell.execute_reply.started":"2021-05-21T08:56:06.351871Z","shell.execute_reply":"2021-05-21T08:56:06.718148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df.iloc[1,])\ntrain_image_list[1]","metadata":{"execution":{"iopub.status.busy":"2021-05-21T08:56:06.72199Z","iopub.execute_input":"2021-05-21T08:56:06.722196Z","iopub.status.idle":"2021-05-21T08:56:07.050025Z","shell.execute_reply.started":"2021-05-21T08:56:06.722155Z","shell.execute_reply":"2021-05-21T08:56:07.049278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/food-101/food-101/meta/test.txt', header=None).apply(lambda x : x + '.jpg')\nprint(\"Total number of samples in test folder:\", len(test_df))\ntest_image_list = ImageItemList.from_df(test_df, '/kaggle/input/food-101/food-101/images')","metadata":{"execution":{"iopub.status.busy":"2021-05-21T08:56:07.050935Z","iopub.execute_input":"2021-05-21T08:56:07.051157Z","iopub.status.idle":"2021-05-21T08:56:07.152422Z","shell.execute_reply.started":"2021-05-21T08:56:07.051121Z","shell.execute_reply":"2021-05-21T08:56:07.151467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_df.iloc[80,])\ntest_image_list[80]","metadata":{"execution":{"iopub.status.busy":"2021-05-21T08:56:07.153882Z","iopub.execute_input":"2021-05-21T08:56:07.154293Z","iopub.status.idle":"2021-05-21T08:56:07.301657Z","shell.execute_reply.started":"2021-05-21T08:56:07.154117Z","shell.execute_reply":"2021-05-21T08:56:07.300758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-process Images\n## Objectives\n1. Implement preprocessing codes for each model. \n2. Augment the dataset. \n3. Preview the preprocessed dataset. ","metadata":{}},{"cell_type":"markdown","source":"### **Pre-processing Steps for TensorFlow (Keras) Model**\n#### The preprocessing steps are done to make sure that the model doesn't overfit the data. In the block below, we have augmented the images by performing random transformations on them. This is extremely useful if the dataset is small in size, but we do it anyways to reduce the chances of overfitting.","metadata":{}},{"cell_type":"code","source":"train_datagen = ImageDataGenerator(\n        rescale=1./255,\n        rotation_range=30,\n        shear_range=0.3,\n        horizontal_flip=True,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        zoom_range=0.25,\n        fill_mode = 'nearest'\n)\nvalid_datagen = ImageDataGenerator(\n        rescale=1./255,\n)","metadata":{"_uuid":"71c58df71a49e4e0e2e0bfe6359602c92c2f2306","id":"9YAscZLV5LFK","execution":{"iopub.status.busy":"2021-05-21T08:56:07.302889Z","iopub.execute_input":"2021-05-21T08:56:07.303261Z","iopub.status.idle":"2021-05-21T08:56:07.310483Z","shell.execute_reply.started":"2021-05-21T08:56:07.303095Z","shell.execute_reply":"2021-05-21T08:56:07.309491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 128\n\ntrain_data = train_datagen.flow_from_directory(\n    '/train/',\n    batch_size=batch_size,\n    target_size=(331, 331),\n    shuffle=True,\n)\nvalid_data = valid_datagen.flow_from_directory(\n    '/test/',\n    target_size=(331, 331),\n    batch_size=batch_size,\n    shuffle=False,\n)","metadata":{"_uuid":"2a6d67301fb0a709613c1c7a3f00629c349065e3","id":"tvlXbJ3NoPzy","outputId":"7a4173c9-9328-421e-e65f-0a25ecf55591","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training different models\n## Objectives\n1. Obtain 90% accuracy in all the models trained. \n2. You're free to use any techniques for traning such as transfer learning, knowledge transfer, etc. \n3. The models should not overfit the training dataset. \n4. Measure the performance in terms of accuracy and speed of each model. \n5. Visualize the training and testing performance using TensorBoard. \n\n#### Optional:\n1. Apply weight quantization to increase the speed of the models. ","metadata":{"_uuid":"b9b7bc7c0a9ca43a9b0a2d1c77e11bd40f305ae9","id":"z5hh8fj8iIaV"}},{"cell_type":"markdown","source":"#### Before starting with the training, I decided that it'd be better to use the models that have already been pretrained on Imagenet. So I decided to go with the Transfer Learning approach here. Below are the points in the order I followed them while finalising the model.\n1. I first headed over the Keras applications page to take a look at the different models and their sizes. they have also listed out the accuracies of all the models over there.\n2. VGG16 and VGG19 were the heaviest models, followed by NasNetLarge, but NasNetLarge showed a much better accuracy compared to them.\n3. MobileNetv2, DenseNet169, and NasNetMobile were the smallest models. However, the accuracy of DenseNet169 was the highest among these three.\n4. I first did a simple comparison between DenseNet169 and MobileNetv2 to find out the one that performed better. DenseNet169 performed better. MobileNetv2 ran in approximately 3h 8m whereas DenseNet169 ran for 3h 17m. DenseNet169's accuracy was around 4% higher compared to MobileNetv2 so I have performed my future calculations using DensenNet169.\n5. After this, I ran two instances on this model, one with SGD (with momentum) and another one with the ADAM optimizer. Their learning rates were kept the same. I found out that ADAM provided better accuracy and hence chose to proceed forward with this optimizer.\n6. I have also added ***Dropout*** as another form of regularization, again, to prevent overfitting of the model. I am also planning to add L2 regularization at the end, I have read that these two perform well together in certain cases.\n7. The reason why I have commented out the ***Tensorboard*** operations is because I was facing some issues while using it on Kaggle and didn't have the time to sort these issues out. However, they worked just fine on Google Colab.","metadata":{}},{"cell_type":"code","source":"os.getcwd()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:04:42.500302Z","iopub.execute_input":"2021-05-21T05:04:42.500586Z","iopub.status.idle":"2021-05-21T05:04:42.50968Z","shell.execute_reply.started":"2021-05-21T05:04:42.500541Z","shell.execute_reply":"2021-05-21T05:04:42.508284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Load the extension and start TensorBoard\n\n# %load_ext tensorboard.notebook\n# %tensorboard --logdir logs","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:04:42.511133Z","iopub.execute_input":"2021-05-21T05:04:42.511672Z","iopub.status.idle":"2021-05-21T05:04:42.518747Z","shell.execute_reply.started":"2021-05-21T05:04:42.511365Z","shell.execute_reply":"2021-05-21T05:04:42.51505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base1 = tf.keras.applications.DenseNet169(input_shape=(331,331,3), include_top=False, weights='imagenet')\nbase1.trainable = False\nmodel1 = Sequential()\nmodel1.add(base1)\n# model1.add(Flatten())\nmodel1.add(GlobalAveragePooling2D())\nmodel1.add(Dense(256, activation='relu'))\nmodel1.add(Dropout(0.5))\nmodel1.add(Dense(101, activation='softmax'))\n# opt = AdaBound(lr=1e-3)\nopt = Adam(lr=0.001)\n# opt = SGD(lr=0.001, momentum = 0.9)\nmodel1.compile(optimizer=opt,loss = 'categorical_crossentropy',metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-05-21T10:07:28.641542Z","iopub.execute_input":"2021-05-21T10:07:28.64183Z","iopub.status.idle":"2021-05-21T10:08:14.76343Z","shell.execute_reply.started":"2021-05-21T10:07:28.641782Z","shell.execute_reply":"2021-05-21T10:08:14.762686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.layers","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:19:30.244235Z","iopub.execute_input":"2021-05-21T09:19:30.244516Z","iopub.status.idle":"2021-05-21T09:19:30.251334Z","shell.execute_reply.started":"2021-05-21T09:19:30.244464Z","shell.execute_reply":"2021-05-21T09:19:30.250644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model1.summary()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:04:42.52735Z","iopub.execute_input":"2021-05-21T05:04:42.527964Z","iopub.status.idle":"2021-05-21T05:04:42.534663Z","shell.execute_reply.started":"2021-05-21T05:04:42.52791Z","shell.execute_reply":"2021-05-21T05:04:42.533879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduce_lr = ReduceLROnPlateau(monitor = 'val_acc',patience = 1,verbose = 1)\nearly_stop = EarlyStopping(monitor = 'val_acc',patience = 5,verbose = 1)\n# log = \"/logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n# tensorboard = TensorBoard(log_dir=log,write_graph = True, update_freq = 'batch')\nchkp1 = ModelCheckpoint('/kaggle/working/NNL.h5',monitor='val_acc',verbose=1,save_best_only=True)\ncsv_logger1 = CSVLogger('/kaggle/working/NNL.log')","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:04:42.536125Z","iopub.execute_input":"2021-05-21T05:04:42.536669Z","iopub.status.idle":"2021-05-21T05:04:42.54375Z","shell.execute_reply.started":"2021-05-21T05:04:42.536618Z","shell.execute_reply":"2021-05-21T05:04:42.542971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nhistory1 = model1.fit(train_data, \n                    epochs=15,\n                    validation_data = valid_data,\n                    callbacks=[early_stop, reduce_lr, csv_logger1, chkp1])","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:04:42.545061Z","iopub.execute_input":"2021-05-21T05:04:42.545608Z","iopub.status.idle":"2021-05-21T05:04:42.552609Z","shell.execute_reply.started":"2021-05-21T05:04:42.545559Z","shell.execute_reply":"2021-05-21T05:04:42.551826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.save('NNL.h5')","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:04:42.554074Z","iopub.execute_input":"2021-05-21T05:04:42.554603Z","iopub.status.idle":"2021-05-21T05:04:42.563914Z","shell.execute_reply.started":"2021-05-21T05:04:42.554556Z","shell.execute_reply":"2021-05-21T05:04:42.563047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Quantized Model\n#### The Quantization used here is known as quantization aware training as it is much better for model accuracy compared to post-training quantization.","metadata":{}},{"cell_type":"code","source":"# import tensorflow_model_optimization as tfmot\n\n# quantize_model = tfmot.quantization.keras.quantize_model\n\n# # q_aware stands for for quantization aware.\n# q_aware_model1 = quantize_model(model1)\n\n# # `quantize_model` requires a recompile.\n# q_aware_model1.compile(optimizer='adam',\n#               loss=tf.keras.losses.CategoricalCrossentropy(),\n#               metrics=['accuracy'])\n\n# q_aware_model1.summary()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:04:42.565282Z","iopub.execute_input":"2021-05-21T05:04:42.565865Z","iopub.status.idle":"2021-05-21T05:04:42.572704Z","shell.execute_reply.started":"2021-05-21T05:04:42.565805Z","shell.execute_reply":"2021-05-21T05:04:42.571736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reduce_lr = ReduceLROnPlateau(monitor = 'val_acc',patience = 1,verbose = 1)\n# early_stop = EarlyStopping(monitor = 'val_acc',patience = 5,verbose = 1)\n# # log = \"/logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n# # tensorboard = TensorBoard(log_dir=log,write_graph = True, update_freq = 'batch')\n# chkp2 = ModelCheckpoint('/kaggle/working/quant_NasNetLarge.h5',monitor='val_acc',verbose=1,save_best_only=True)\n# csv_logger2 = CSVLogger('/kaggle/working/quant_NasNetLarge.log')","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:04:42.574122Z","iopub.execute_input":"2021-05-21T05:04:42.574625Z","iopub.status.idle":"2021-05-21T05:04:42.580734Z","shell.execute_reply.started":"2021-05-21T05:04:42.57458Z","shell.execute_reply":"2021-05-21T05:04:42.58Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n\n# q_aware_model.fit(train_data, validation_data = valid_data, epochs=15, \n#                   callbacks=[early_stop, reduce_lr, csv_logger2, chkp2])","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:04:42.58207Z","iopub.execute_input":"2021-05-21T05:04:42.582634Z","iopub.status.idle":"2021-05-21T05:04:42.589953Z","shell.execute_reply.started":"2021-05-21T05:04:42.582586Z","shell.execute_reply":"2021-05-21T05:04:42.589112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# q_aware_model.save('NasNetLarge_quantized.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine Tuning Weights\n### After training the model, we can take the best weights from it and try unfreezing some layers from the tranfer learning model used to improve accuracy.","metadata":{}},{"cell_type":"code","source":"model1.load_weights('/kaggle/working/NNL.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(model1.layers)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T10:08:21.951561Z","iopub.execute_input":"2021-05-21T10:08:21.951842Z","iopub.status.idle":"2021-05-21T10:08:21.958981Z","shell.execute_reply.started":"2021-05-21T10:08:21.95179Z","shell.execute_reply":"2021-05-21T10:08:21.958247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for layer in model1.layers[:300]:\n        layer.trainable = False\n    for layer in model1.layers[300:]:\n        layer.trainable = True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.compile(optimizer='adam',\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduce_lr = ReduceLROnPlateau(monitor = 'val_acc',patience = 1,verbose = 1)\nearly_stop = EarlyStopping(monitor = 'val_acc',patience = 5,verbose = 1)\n# log = \"/logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n# tensorboard = TensorBoard(log_dir=log,write_graph = True, update_freq = 'batch')\nchkp3 = ModelCheckpoint('/kaggle/working/NNL_finetune.h5',monitor='val_acc',verbose=1,save_best_only=True)\ncsv_logger3 = CSVLogger('/kaggle/working/NNL_finetune.log')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nhistory1 = model1.fit(train_data, \n                    epochs=15,\n                    validation_data = valid_data,\n                    callbacks=[early_stop, reduce_lr, csv_logger1, chkp1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.save('NNL_finetune.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Visualize the accuracy and loss plots**","metadata":{"_uuid":"732521c21ce35b5be0d571cbe9d392d93755671c","id":"KbDzLAHGpJXQ"}},{"cell_type":"code","source":"# def plot_accuracy(history,title):\n#     plt.title(title)\n#     plt.plot(history.history['acc'])\n#     plt.plot(history.history['val_acc'])\n#     plt.ylabel('accuracy')\n#     plt.xlabel('epoch')\n#     plt.legend(['train_accuracy', 'validation_accuracy'], loc='best')\n#     plt.show()\n# def plot_loss(history,title):\n#     plt.title(title)\n#     plt.plot(history.history['loss'])\n#     plt.plot(history.history['val_loss'])\n#     plt.ylabel('loss')\n#     plt.xlabel('epoch')\n#     plt.legend(['train_loss', 'validation_loss'], loc='best')\n#     plt.show()\n","metadata":{"_uuid":"27084e072edb2c961bfa6be87d80f273d32533c2","id":"SjRm_AWZpPZm","outputId":"42a0123b-d15d-4ffa-cba4-e92cd81b0324","execution":{"iopub.status.busy":"2021-05-21T05:04:42.59125Z","iopub.execute_input":"2021-05-21T05:04:42.591801Z","iopub.status.idle":"2021-05-21T05:04:42.598434Z","shell.execute_reply.started":"2021-05-21T05:04:42.591693Z","shell.execute_reply":"2021-05-21T05:04:42.597583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot_accuracy(history,'FOOD101-MobileNetv2')\n# plot_loss(history,'FOOD101-MobileNetv2')","metadata":{"_uuid":"a73c830d2fbd841c9541628061313e4fd8506a51","execution":{"iopub.status.busy":"2021-05-21T05:04:42.599672Z","iopub.execute_input":"2021-05-21T05:04:42.600324Z","iopub.status.idle":"2021-05-21T05:04:42.607877Z","shell.execute_reply.started":"2021-05-21T05:04:42.600275Z","shell.execute_reply":"2021-05-21T05:04:42.606983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Preprocessing steps for PyTorch Model**","metadata":{"_uuid":"16347094046271c17a55125f5ff6bc002edc6152","id":"qjeWMHrCwSoS"}},{"cell_type":"code","source":"# train_transforms = torchvision.transforms.Compose([\n#         torchvision.transforms.ColorJitter(brightness=0.1,contrast=0.1,saturation=0.1),\n#         torchvision.transforms.RandomAffine(15),\n#         torchvision.transforms.RandomHorizontalFlip(),\n#         torchvision.transforms.RandomRotation(15),\n#         torchvision.transforms.Resize((224,224)),\n#         torchvision.transforms.ToTensor(),\n#         torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n# ])\n# valid_transforms = torchvision.transforms.Compose([\n#         torchvision.transforms.Resize((224,224)),\n#         torchvision.transforms.ToTensor(),\n#         torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n# ])","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:04:05.592922Z","iopub.execute_input":"2021-05-21T09:04:05.593194Z","iopub.status.idle":"2021-05-21T09:04:05.610179Z","shell.execute_reply.started":"2021-05-21T09:04:05.593148Z","shell.execute_reply":"2021-05-21T09:04:05.609253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def get_device():\n#     if torch.cuda.is_available():\n#         return torch.device(\"cuda\")\n#     else:\n#         return torch.device(\"cpu\")\n    \n# def to_device(data, device):\n#     if isinstance(data, (list,tuple)):\n#         return [to_device(x, device) for x in data]\n#     return data.to(device, non_blocking=True)\n\n# class DeviceDataLoader():\n#     def __init__(self, dl, device):\n#         self.dl = dl\n#         self.device = device\n        \n#     def __iter__(self):\n#         for x in self.dl:\n#             yield to_device(x, self.device)\n            \n#     def __len__(self):\n#         return len(self.dl)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:04:07.402953Z","iopub.execute_input":"2021-05-21T09:04:07.403235Z","iopub.status.idle":"2021-05-21T09:04:07.409011Z","shell.execute_reply.started":"2021-05-21T09:04:07.403177Z","shell.execute_reply":"2021-05-21T09:04:07.408307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# device = get_device()\n# device","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_dataset = torchvision.datasets.ImageFolder('/train/',transform=train_transforms)\n# valid_dataset = torchvision.datasets.ImageFolder('/test/',transform=valid_transforms)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:04:10.998909Z","iopub.execute_input":"2021-05-21T09:04:10.999179Z","iopub.status.idle":"2021-05-21T09:04:11.503801Z","shell.execute_reply.started":"2021-05-21T09:04:10.999131Z","shell.execute_reply":"2021-05-21T09:04:11.503124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# batch_size = 128\n# train_loader = torch.utils.data.DataLoader(train_dataset,batch_size,shuffle=True,num_workers=0,pin_memory=True)\n# valid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size,shuffle=False,num_workers=0,pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:04:13.387302Z","iopub.execute_input":"2021-05-21T09:04:13.387578Z","iopub.status.idle":"2021-05-21T09:04:13.393899Z","shell.execute_reply.started":"2021-05-21T09:04:13.387532Z","shell.execute_reply":"2021-05-21T09:04:13.391219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_dl = DeviceDataLoader(train_loader, device)\n# val_dl = DeviceDataLoader(valid_loader, device)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:04:23.593768Z","iopub.execute_input":"2021-05-21T09:04:23.594037Z","iopub.status.idle":"2021-05-21T09:04:23.598487Z","shell.execute_reply.started":"2021-05-21T09:04:23.593993Z","shell.execute_reply":"2021-05-21T09:04:23.597277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PyTorch Light Model","metadata":{}},{"cell_type":"code","source":"# mobilenet_v3_small = modelst.mobilenet_v3_small()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:15:21.996331Z","iopub.execute_input":"2021-05-21T09:15:21.996647Z","iopub.status.idle":"2021-05-21T09:15:22.014607Z","shell.execute_reply.started":"2021-05-21T09:15:21.996596Z","shell.execute_reply":"2021-05-21T09:15:22.013612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_l = torchvision.models.mobilenet_v2(pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:13:21.382892Z","iopub.execute_input":"2021-05-21T09:13:21.383169Z","iopub.status.idle":"2021-05-21T09:13:21.402406Z","shell.execute_reply.started":"2021-05-21T09:13:21.383119Z","shell.execute_reply":"2021-05-21T09:13:21.401414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_l","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The below block function is used to freeze the number of layers ","metadata":{}},{"cell_type":"code","source":"# for i,param in enumerate(model_l.parameters()):\n#     if i<169:\n#         param.requires_grad=False","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:07:08.452258Z","iopub.execute_input":"2021-05-21T09:07:08.452578Z","iopub.status.idle":"2021-05-21T09:07:08.459599Z","shell.execute_reply.started":"2021-05-21T09:07:08.452525Z","shell.execute_reply":"2021-05-21T09:07:08.458038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_l.fc = torch.nn.Sequential(\n#     torch.nn.Dropout(0.5),\n#     torch.nn.Linear(2048,101)\n# )","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:07:09.976243Z","iopub.execute_input":"2021-05-21T09:07:09.976536Z","iopub.status.idle":"2021-05-21T09:07:09.983972Z","shell.execute_reply.started":"2021-05-21T09:07:09.976484Z","shell.execute_reply":"2021-05-21T09:07:09.983281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_l = to_device(model_l, device)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:07:10.944273Z","iopub.execute_input":"2021-05-21T09:07:10.944598Z","iopub.status.idle":"2021-05-21T09:07:10.994622Z","shell.execute_reply.started":"2021-05-21T09:07:10.94455Z","shell.execute_reply":"2021-05-21T09:07:10.994002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The summary function gives us the total number of layers in a model which helps us decide the number of layers we want to freeze.","metadata":{}},{"cell_type":"code","source":"# summary(model_l, (3, 224, 224))","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:07:11.912404Z","iopub.execute_input":"2021-05-21T09:07:11.912702Z","iopub.status.idle":"2021-05-21T09:07:12.127557Z","shell.execute_reply.started":"2021-05-21T09:07:11.912654Z","shell.execute_reply":"2021-05-21T09:07:12.126725Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### This piece of block below helps us find the optimal learning rate by taking into account the steepest gradient","metadata":{}},{"cell_type":"code","source":"# from torch_lr_finder import LRFinder\n# criterion = torch.nn.CrossEntropyLoss()\n# optimizer = torch.optim.Adam(model_l.parameters(), lr=0.00001)\n# lr_finder = LRFinder(model_l, optimizer, criterion, device=\"cuda\")\n# lr_finder.range_test(train_loader, end_lr=0.001, num_iter=25)\n# lr_finder.plot()\n# lr_finder.reset()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n\n# cuda = True\n# epochs = 10\n# model_name = '/kaggle/working/densenet161.pt'\n# optimizer = torch.optim.Adam(model_l.parameters(),lr=3.83e-4,weight_decay=0.001)\n# criterion = torch.nn.CrossEntropyLoss()\n# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.1,patience=1,verbose=True)\n\n# # writer = SummaryWriter() # For Tensorboard\n# early_stop_count=0\n# ES_patience=5\n# best = 0.0\n# if cuda:\n#     model_l.cuda()\n\n# for epoch in range(epochs):\n    \n#     # Training\n#     model_l.train()\n#     correct = 0\n#     train_loss = 0.0\n#     tbar = tqdm(train_loader, desc = 'Training', position=0, leave=True)\n#     for i,(inp,lbl) in enumerate(tbar):\n#         optimizer.zero_grad()\n#         if cuda:\n#             inp,lbl = inp.cuda(),lbl.cuda()\n#         out = model_l(inp)\n#         loss = criterion(out,lbl)\n#         train_loss += loss\n#         out = out.argmax(dim=1)\n#         correct += (out == lbl).sum().item()\n#         loss.backward()\n#         optimizer.step()\n#         tbar.set_description(f\"Epoch: {epoch+1}, loss: {loss.item():.5f}, acc: {100.0*correct/((i+1)*train_loader.batch_size):.4f}%\")\n#     train_acc = 100.0*correct/len(train_loader.dataset)\n#     train_loss /= (len(train_loader.dataset)/batch_size)\n\n#     # Validation\n#     model_l.eval()\n#     with torch.no_grad():\n#         correct = 0\n#         val_loss = 0.0\n#         vbar = tqdm(valid_loader, desc = 'Validation', position=0, leave=True)\n#         for i,(inp,lbl) in enumerate(vbar):\n#             if cuda:\n#                 inp,lbl = inp.cuda(),lbl.cuda()\n#             out = model_l(inp)\n#             val_loss += criterion(out,lbl)\n#             out = out.argmax(dim=1)\n#             correct += (out == lbl).sum().item()\n#         val_acc = 100.0*correct/len(valid_loader.dataset)\n#         val_loss /= (len(valid_loader.dataset)/batch_size)\n#     print(f'\\nEpoch: {epoch+1}/{epochs}')\n#     print(f'Train loss: {train_loss}, Train Accuracy: {train_acc}')\n#     print(f'Validation loss: {val_loss}, Validation Accuracy: {val_acc}\\n')\n\n#     scheduler.step(val_loss)\n\n#     # write to tensorboard\n#     #writer.add_scalar(\"Loss/train\", train_loss, epoch)\n#     #writer.add_scalar(\"Loss/val\", val_loss, epoch)\n#     #writer.add_scalar(\"Accuracy/train\", train_acc, epoch)\n#     #writer.add_scalar(\"Accuracy/val\", val_acc, epoch)\n\n#     if val_acc>best:\n#         best=val_acc\n#         torch.save( model_l,model_name)\n#         early_stop_count=0\n#         print('Accuracy Improved, model saved.\\n')\n#     else:\n#         early_stop_count+=1\n\n#     if early_stop_count==ES_patience:\n#         print('Early Stopping Initiated...')\n#         print(f'Best Accuracy achieved: {best:.2f}% at epoch:{epoch-ES_patience}')\n#         print(f'Model saved as {model_name}')\n#         break\n#     #writer.flush()\n# # writer.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PyTorch Medium/Heavy Model","metadata":{}},{"cell_type":"code","source":"# model_t = resnet50(pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:06:25.03866Z","iopub.execute_input":"2021-05-21T09:06:25.038934Z","iopub.status.idle":"2021-05-21T09:06:27.533689Z","shell.execute_reply.started":"2021-05-21T09:06:25.038888Z","shell.execute_reply":"2021-05-21T09:06:27.53292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_t","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for i,param in enumerate(model_t.parameters()):\n#     if i<169:\n#         param.requires_grad=False","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:06:39.372867Z","iopub.execute_input":"2021-05-21T09:06:39.373142Z","iopub.status.idle":"2021-05-21T09:06:39.377782Z","shell.execute_reply.started":"2021-05-21T09:06:39.373092Z","shell.execute_reply":"2021-05-21T09:06:39.376912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_t.fc = torch.nn.Sequential(\n#     torch.nn.Dropout(0.5),\n#     torch.nn.Linear(2048,101)\n# )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_t = to_device(model_t, device)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:06:44.816254Z","iopub.execute_input":"2021-05-21T09:06:44.816536Z","iopub.status.idle":"2021-05-21T09:06:44.850755Z","shell.execute_reply.started":"2021-05-21T09:06:44.816486Z","shell.execute_reply":"2021-05-21T09:06:44.85002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summary(model_t, (3, 224, 224))","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### From the below summary, we can see that there are a total of 175 layers including the ones that we added, hence, we freeze the weights of the first 173 layers as we are only want the final layers' weights to be updated.","metadata":{}},{"cell_type":"code","source":"# torch.cuda.is_available()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from torch_lr_finder import LRFinder\n# criterion = torch.nn.CrossEntropyLoss()\n# optimizer = torch.optim.Adam(model_t.parameters(), lr=0.00001)\n# lr_finder = LRFinder(model_t, optimizer, criterion, device=\"cuda\")\n# lr_finder.range_test(train_loader, end_lr=0.001, num_iter=25)\n# lr_finder.plot()\n# lr_finder.reset()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T06:39:24.872028Z","iopub.execute_input":"2021-05-21T06:39:24.872423Z","iopub.status.idle":"2021-05-21T06:40:29.441132Z","shell.execute_reply.started":"2021-05-21T06:39:24.872245Z","shell.execute_reply":"2021-05-21T06:40:29.440546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_t.parameters()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:04:54.68249Z","iopub.status.idle":"2021-05-21T05:04:54.68324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n\n# cuda = True\n# epochs = 10\n# model_name = '/kaggle/working/resnet50.pt'\n# optimizer = torch.optim.Adam(model_t.parameters(),lr=3.83e-4,weight_decay=0.001)\n# criterion = torch.nn.CrossEntropyLoss()\n# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.1,patience=1,verbose=True)\n\n# # writer = SummaryWriter() # For Tensorboard\n# early_stop_count=0\n# ES_patience=5\n# best = 0.0\n# if cuda:\n#     model_t.cuda()\n\n# for epoch in range(epochs):\n    \n#     # Training\n#     model_t.train()\n#     correct = 0\n#     train_loss = 0.0\n#     tbar = tqdm(train_loader, desc = 'Training', position=0, leave=True)\n#     for i,(inp,lbl) in enumerate(tbar):\n#         optimizer.zero_grad()\n#         if cuda:\n#             inp,lbl = inp.cuda(),lbl.cuda()\n#         out = model_t(inp)\n#         loss = criterion(out,lbl)\n#         train_loss += loss\n#         out = out.argmax(dim=1)\n#         correct += (out == lbl).sum().item()\n#         loss.backward()\n#         optimizer.step()\n#         tbar.set_description(f\"Epoch: {epoch+1}, loss: {loss.item():.5f}, acc: {100.0*correct/((i+1)*train_loader.batch_size):.4f}%\")\n#     train_acc = 100.0*correct/len(train_loader.dataset)\n#     train_loss /= (len(train_loader.dataset)/batch_size)\n\n#     # Validation\n#     model_t.eval()\n#     with torch.no_grad():\n#         correct = 0\n#         val_loss = 0.0\n#         vbar = tqdm(valid_loader, desc = 'Validation', position=0, leave=True)\n#         for i,(inp,lbl) in enumerate(vbar):\n#             if cuda:\n#                 inp,lbl = inp.cuda(),lbl.cuda()\n#             out = model_t(inp)\n#             val_loss += criterion(out,lbl)\n#             out = out.argmax(dim=1)\n#             correct += (out == lbl).sum().item()\n#         val_acc = 100.0*correct/len(valid_loader.dataset)\n#         val_loss /= (len(valid_loader.dataset)/batch_size)\n#     print(f'\\nEpoch: {epoch+1}/{epochs}')\n#     print(f'Train loss: {train_loss}, Train Accuracy: {train_acc}')\n#     print(f'Validation loss: {val_loss}, Validation Accuracy: {val_acc}\\n')\n\n#     scheduler.step(val_loss)\n\n#     # write to tensorboard\n#     #writer.add_scalar(\"Loss/train\", train_loss, epoch)\n#     #writer.add_scalar(\"Loss/val\", val_loss, epoch)\n#     #writer.add_scalar(\"Accuracy/train\", train_acc, epoch)\n#     #writer.add_scalar(\"Accuracy/val\", val_acc, epoch)\n\n#     if val_acc>best:\n#         best=val_acc\n#         torch.save( model_t,model_name)\n#         early_stop_count=0\n#         print('Accuracy Improved, model saved.\\n')\n#     else:\n#         early_stop_count+=1\n\n#     if early_stop_count==ES_patience:\n#         print('Early Stopping Initiated...')\n#         print(f'Best Accuracy achieved: {best:.2f}% at epoch:{epoch-ES_patience}')\n#         print(f'Model saved as {model_name}')\n#         break\n#     #writer.flush()\n# # writer.close()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T07:20:21.560055Z","iopub.execute_input":"2021-05-21T07:20:21.560382Z","iopub.status.idle":"2021-05-21T07:37:14.626337Z","shell.execute_reply.started":"2021-05-21T07:20:21.560315Z","shell.execute_reply":"2021-05-21T07:37:14.625504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n1. https://keras.io/api/applications/\n2. https://www.tensorflow.org/model_optimization/guide/quantization/training\n3. https://www.kaggle.com/pranshu15/tensorflow-keras-mobilenetv2-77\n4. https://pytorch.org/vision/stable/models.html\n5. https://pypi.org/project/keras-adabound/","metadata":{}}]}